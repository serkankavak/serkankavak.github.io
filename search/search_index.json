{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Some quick tips","title":"Home"},{"location":"#home","text":"Some quick tips","title":"Home"},{"location":"Notes/AI-in-General/","text":"AI in General All the information I need for AI LSTM Predict Stock Prices Predict Stock Prices - 2 Predict Stock Prices - 3 Deep Learning Comprehensive deep learning course from a Taiwanese Prof: Hung-yi Lee 3 Different Ways for Deep Learning in Tensorflow 2.0 1. Sequential Model: The easiest way to get up and running with Keras in TensorFlow 2.0 2. Functional: For more complex models, in particular model with multiple inputs or outputs. 3. Model Subclassing: Fully-customizable and enables us to implement our own custom forward-pass of the model Data Preparation Part import tensorflow as tf import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split iris = load_iris() # Load data into a DataFrame df = pd.DataFrame(iris.data, columns=iris.feature_names) # Convert datatype to float df = df.astype(float) # append \"target\" and name it \"label\" df['label'] = iris.target # Use string label instead df['label'] = df.label.replace(dict(enumerate(iris.target_names))) # label -> one-hot encoding label = pd.get_dummies(df['label']) label.columns = ['label_' + str(x) for x in label.columns] df = pd.concat([df, label], axis=1) # drop old label df.drop(['label'], axis=1, inplace=True) # Creating X and y X = df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']] # Convert DataFrame into np array X = np.asarray(X) y = df[['label_setosa', 'label_versicolor', 'label_virginica']] # Convert DataFrame into np array y = np.asarray(y) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) 1.Sequential Model from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense # Passing a list of layers to the constructor model = Sequential([ Dense(5, activation='relu', input_shape=(4,)), Dense(10, activation='relu'), Dense(3, activation='softmax')]) model.summary() Above is identical to the below: # Adding layer via add() method model = Sequential() model.add(Dense(5, activation='relu', input_shape=(4,))) model.add(Dense(10, activation='relu')) model.add(Dense(3, activation='softmax')) model.summary() Training model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(X_train, y_train, batch_size= 64, epochs= 30, validation_split=0.2) Plot the traning, validation loss and training, validation accuracy def plot_metric(history, metric): train_metrics = history.history[metric] val_metrics = history.history['val_'+metric] epochs = range(1, len(train_metrics) + 1) plt.plot(epochs, train_metrics, 'bo--') plt.plot(epochs, val_metrics, 'ro-') plt.title('Training and validation '+ metric) plt.xlabel(\"Epochs\") plt.ylabel(metric) plt.legend([\"train_\"+metric, 'val_'+metric]) plt.show() plot_metric(history, 'loss') plot_metric(history, 'accuracy') Evaluate the model with test data: model.evaluate(x = X_test,y = y_test) Output: 2/2 [==============================] - 0s 997us/step - loss: 0.7898 - accuracy: 0.7632 [0.7897533774375916, 0.7631579041481018] When to use Sequential Model A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input and one output. A Sequential model is not appropriate when: 1. Your model has multiple inputs or multiple outputs 2. Any of your layers have multiple inputs or multiple outputs 3. You need to do layer sharing 4. You want non-linear topology (e.g. a residual connection, a multi-branch model) 2.Functional API Same neural network with Functional API: from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dense # This returns a tensor inputs = Input(shape=(4,)) # A layer instance is callable on a tensor, and returns a tensor x = Dense(5, activation='relu')(inputs) x = Dense(10, activation='relu')(x) outputs = Dense(3, activation='softmax')(x) # This creates a model that includes # the Input layer and three Dense layers model = Model(inputs=inputs, outputs=outputs) model.summary() Training: same with sequential model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(X_train, y_train, batch_size= 64, epochs= 30, validation_split=0.2) Use the same function to plot loss and accuracy: plot_metric(history, 'loss') plot_metric(history, 'accuracy') Evaluate the model: model.evaluate(x = X_test,y = y_test) Output: 2/2 [==============================] - 0s 998us/step - loss: 0.7550 - accuracy: 0.9474 [0.7550188302993774, 0.9473684430122375] Multi-input and Multi-output Models Functional API is useful for scenarios like: - A model with 2 inputs and 1 output - A model with 1 input and 2 outputs - A model with 2 input and 2 outputs 2 inputs and 1 output: Suppose we have image data and structured data for iris flower classification. We would like to build a Machine Learning model like below: from tensorflow.keras.layers import concatenate # 2 inputs, one structure data, the other image data input_structure = Input(shape=(4,), name='input_structure') input_image = Input(shape=(256,), name='input_image') # middle layers x_1 = Dense(10, activation='relu')(input_structure) x_2 = Dense(100, activation='relu')(input_image) c = concatenate([x_1, x_2]) outputs = Dense(3, activation='softmax', name='outputs')(c) model = Model(inputs=[input_structure, input_image], outputs=outputs) model.summary() 1 input and 2 outputs: Suppose we only have image data and we want to identify if given image is an iris flower and what kind of iris flower it is. from tensorflow.keras.layers import concatenate # only one input input_image = Input(shape=(256,), name='input_image') # middle layer x = Dense(300, activation='relu')(input_image) # output layer output_1 = Dense(1, activation='sigmoid', name='output_1')(x) output_2 = Dense(3, activation='softmax', name='output_2')(x) model = Model(inputs=input_image, outputs=[output_1, output_2]) model.summary() 2 inputs and 2 outputs: Suppose we have image data and structured data. We want to identify if given image is an iris flower and what kind of iris flower it is. from tensorflow.keras.layers import concatenate # 2 inputs, one structured data, the other image data input_structured = Input(shape=(4,), name='input_structured') input_image = Input(shape=(256,), name='input_image') # middle layers x_1 = Dense(10, activation='relu')(input_structure) x_2 = Dense(300, activation='relu')(input_image) c = concatenate([x_1, x_2]) # output layser output_1 = Dense(1, activation='sigmoid', name='output_1')(c) output_2 = Dense(3, activation='softmax', name='output_2')(c) model = Model(inputs=[input_structured, input_image], outputs=[output_1, output_2]) model.summary() 3. Model Subclassing This way of building models gives a low level control over both the construction and the operation of a model. However, it is way harder to utilize it than the Sequential Model and Functional API. from tensorflow.keras.models import Model from tensorflow.keras.layers import Dense class CustomModel(Model): def __init__(self, **kwargs): super(CustomModel, self).__init__(**kwargs) self.dense1 = Dense(5, activation='relu', ) self.dense2 = Dense(10, activation='relu') self.dense3 = Dense(3, activation='softmax') def call(self, inputs): x = self.dense1(inputs) x = self.dense2(x) return self.dense3(x) my_custom_model = CustomModel(name='my_custom_model') CustomModel inherits from the Model class, which is the same Model class that Sequential and Functional API inherit. The important point of Model subclassing is that we create layers in the initializer __init__() and define the forward pass in the call() method. Training and Evaluation same as the others: my_custom_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) history = my_custom_model.fit(X_train, y_train, batch_size= 64, epochs= 30, validation_split=0.2) plot_metric(history, 'loss') plot_metric(history, 'accuracy') my_custom_model.evaluate(x = X_test,y = y_test) Tensorflow Pruning in Keras Make model smaller: Pruning","title":"AI in General"},{"location":"Notes/AI-in-General/#ai-in-general","text":"All the information I need for AI","title":"AI in General"},{"location":"Notes/AI-in-General/#lstm","text":"Predict Stock Prices Predict Stock Prices - 2 Predict Stock Prices - 3","title":"LSTM"},{"location":"Notes/AI-in-General/#deep-learning","text":"Comprehensive deep learning course from a Taiwanese Prof: Hung-yi Lee","title":"Deep Learning"},{"location":"Notes/AI-in-General/#3-different-ways-for-deep-learning-in-tensorflow-20","text":"1. Sequential Model: The easiest way to get up and running with Keras in TensorFlow 2.0 2. Functional: For more complex models, in particular model with multiple inputs or outputs. 3. Model Subclassing: Fully-customizable and enables us to implement our own custom forward-pass of the model Data Preparation Part import tensorflow as tf import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split iris = load_iris() # Load data into a DataFrame df = pd.DataFrame(iris.data, columns=iris.feature_names) # Convert datatype to float df = df.astype(float) # append \"target\" and name it \"label\" df['label'] = iris.target # Use string label instead df['label'] = df.label.replace(dict(enumerate(iris.target_names))) # label -> one-hot encoding label = pd.get_dummies(df['label']) label.columns = ['label_' + str(x) for x in label.columns] df = pd.concat([df, label], axis=1) # drop old label df.drop(['label'], axis=1, inplace=True) # Creating X and y X = df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']] # Convert DataFrame into np array X = np.asarray(X) y = df[['label_setosa', 'label_versicolor', 'label_virginica']] # Convert DataFrame into np array y = np.asarray(y) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","title":"3 Different Ways for Deep Learning in Tensorflow 2.0"},{"location":"Notes/AI-in-General/#1sequential-model","text":"from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense # Passing a list of layers to the constructor model = Sequential([ Dense(5, activation='relu', input_shape=(4,)), Dense(10, activation='relu'), Dense(3, activation='softmax')]) model.summary() Above is identical to the below: # Adding layer via add() method model = Sequential() model.add(Dense(5, activation='relu', input_shape=(4,))) model.add(Dense(10, activation='relu')) model.add(Dense(3, activation='softmax')) model.summary() Training model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(X_train, y_train, batch_size= 64, epochs= 30, validation_split=0.2) Plot the traning, validation loss and training, validation accuracy def plot_metric(history, metric): train_metrics = history.history[metric] val_metrics = history.history['val_'+metric] epochs = range(1, len(train_metrics) + 1) plt.plot(epochs, train_metrics, 'bo--') plt.plot(epochs, val_metrics, 'ro-') plt.title('Training and validation '+ metric) plt.xlabel(\"Epochs\") plt.ylabel(metric) plt.legend([\"train_\"+metric, 'val_'+metric]) plt.show() plot_metric(history, 'loss') plot_metric(history, 'accuracy') Evaluate the model with test data: model.evaluate(x = X_test,y = y_test) Output: 2/2 [==============================] - 0s 997us/step - loss: 0.7898 - accuracy: 0.7632 [0.7897533774375916, 0.7631579041481018]","title":"1.Sequential Model"},{"location":"Notes/AI-in-General/#when-to-use-sequential-model","text":"A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input and one output. A Sequential model is not appropriate when: 1. Your model has multiple inputs or multiple outputs 2. Any of your layers have multiple inputs or multiple outputs 3. You need to do layer sharing 4. You want non-linear topology (e.g. a residual connection, a multi-branch model)","title":"When to use Sequential Model"},{"location":"Notes/AI-in-General/#2functional-api","text":"Same neural network with Functional API: from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Dense # This returns a tensor inputs = Input(shape=(4,)) # A layer instance is callable on a tensor, and returns a tensor x = Dense(5, activation='relu')(inputs) x = Dense(10, activation='relu')(x) outputs = Dense(3, activation='softmax')(x) # This creates a model that includes # the Input layer and three Dense layers model = Model(inputs=inputs, outputs=outputs) model.summary() Training: same with sequential model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(X_train, y_train, batch_size= 64, epochs= 30, validation_split=0.2) Use the same function to plot loss and accuracy: plot_metric(history, 'loss') plot_metric(history, 'accuracy') Evaluate the model: model.evaluate(x = X_test,y = y_test) Output: 2/2 [==============================] - 0s 998us/step - loss: 0.7550 - accuracy: 0.9474 [0.7550188302993774, 0.9473684430122375]","title":"2.Functional API"},{"location":"Notes/AI-in-General/#multi-input-and-multi-output-models","text":"Functional API is useful for scenarios like: - A model with 2 inputs and 1 output - A model with 1 input and 2 outputs - A model with 2 input and 2 outputs 2 inputs and 1 output: Suppose we have image data and structured data for iris flower classification. We would like to build a Machine Learning model like below: from tensorflow.keras.layers import concatenate # 2 inputs, one structure data, the other image data input_structure = Input(shape=(4,), name='input_structure') input_image = Input(shape=(256,), name='input_image') # middle layers x_1 = Dense(10, activation='relu')(input_structure) x_2 = Dense(100, activation='relu')(input_image) c = concatenate([x_1, x_2]) outputs = Dense(3, activation='softmax', name='outputs')(c) model = Model(inputs=[input_structure, input_image], outputs=outputs) model.summary() 1 input and 2 outputs: Suppose we only have image data and we want to identify if given image is an iris flower and what kind of iris flower it is. from tensorflow.keras.layers import concatenate # only one input input_image = Input(shape=(256,), name='input_image') # middle layer x = Dense(300, activation='relu')(input_image) # output layer output_1 = Dense(1, activation='sigmoid', name='output_1')(x) output_2 = Dense(3, activation='softmax', name='output_2')(x) model = Model(inputs=input_image, outputs=[output_1, output_2]) model.summary() 2 inputs and 2 outputs: Suppose we have image data and structured data. We want to identify if given image is an iris flower and what kind of iris flower it is. from tensorflow.keras.layers import concatenate # 2 inputs, one structured data, the other image data input_structured = Input(shape=(4,), name='input_structured') input_image = Input(shape=(256,), name='input_image') # middle layers x_1 = Dense(10, activation='relu')(input_structure) x_2 = Dense(300, activation='relu')(input_image) c = concatenate([x_1, x_2]) # output layser output_1 = Dense(1, activation='sigmoid', name='output_1')(c) output_2 = Dense(3, activation='softmax', name='output_2')(c) model = Model(inputs=[input_structured, input_image], outputs=[output_1, output_2]) model.summary()","title":"Multi-input and Multi-output Models"},{"location":"Notes/AI-in-General/#3-model-subclassing","text":"This way of building models gives a low level control over both the construction and the operation of a model. However, it is way harder to utilize it than the Sequential Model and Functional API. from tensorflow.keras.models import Model from tensorflow.keras.layers import Dense class CustomModel(Model): def __init__(self, **kwargs): super(CustomModel, self).__init__(**kwargs) self.dense1 = Dense(5, activation='relu', ) self.dense2 = Dense(10, activation='relu') self.dense3 = Dense(3, activation='softmax') def call(self, inputs): x = self.dense1(inputs) x = self.dense2(x) return self.dense3(x) my_custom_model = CustomModel(name='my_custom_model') CustomModel inherits from the Model class, which is the same Model class that Sequential and Functional API inherit. The important point of Model subclassing is that we create layers in the initializer __init__() and define the forward pass in the call() method. Training and Evaluation same as the others: my_custom_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) history = my_custom_model.fit(X_train, y_train, batch_size= 64, epochs= 30, validation_split=0.2) plot_metric(history, 'loss') plot_metric(history, 'accuracy') my_custom_model.evaluate(x = X_test,y = y_test)","title":"3. Model Subclassing"},{"location":"Notes/AI-in-General/#tensorflow-pruning-in-keras","text":"Make model smaller: Pruning","title":"Tensorflow Pruning in Keras"},{"location":"Notes/Computer-Science-in-General/","text":"CS in General","title":"CS in General"},{"location":"Notes/Computer-Science-in-General/#cs-in-general","text":"","title":"CS in General"},{"location":"Notes/Linux-in-General/","text":"Linux in General Here is my Linux cheat sheet, mainly on Ubuntu 18.04 Download: http://ubuntu.cs.nctu.edu.tw/ubuntu-release/18.04.4/ CUDA & cuDNN Installation CUDA Installation cuDNN Installation and test Useful Commands List Users cat /etc/passwd Use it to see all the informations about each user. Each line in the file has seven fields delimited by colons that contain the following information: User name Encrypted password (x means that the password is stored in the /etc/shadow file) User ID number (UID) User\u2019s group ID number (GID) Full name of the user (GECOS) User home directory Login shell (defaults to /bin/bash) Another way: getent passwd To display only the usernames: cut -d: -f1 /etc/passwd or getent passwd | cut -d: -f1 To check if a user exists in the Linux system: getent passwd | grep username Count the number of users: getent passwd | wc -l Task Manager in Ubuntu: gnome-system-monitor or htop The address of an installed application after sudo apt-get install: dpkg -L <packagename> Record your voice with microphone Check the volume of your microphone first from alsamixer Record your voice: rec test.wav Ctrl+C to finish recording Firewall with UFW General commands for UFW Deny ICMP ping request","title":"Linux in General"},{"location":"Notes/Linux-in-General/#linux-in-general","text":"Here is my Linux cheat sheet, mainly on Ubuntu 18.04 Download: http://ubuntu.cs.nctu.edu.tw/ubuntu-release/18.04.4/","title":"Linux in General"},{"location":"Notes/Linux-in-General/#cuda-cudnn-installation","text":"CUDA Installation cuDNN Installation and test","title":"CUDA &amp; cuDNN Installation"},{"location":"Notes/Linux-in-General/#useful-commands","text":"","title":"Useful Commands"},{"location":"Notes/Linux-in-General/#list-users","text":"cat /etc/passwd Use it to see all the informations about each user. Each line in the file has seven fields delimited by colons that contain the following information: User name Encrypted password (x means that the password is stored in the /etc/shadow file) User ID number (UID) User\u2019s group ID number (GID) Full name of the user (GECOS) User home directory Login shell (defaults to /bin/bash) Another way: getent passwd To display only the usernames: cut -d: -f1 /etc/passwd or getent passwd | cut -d: -f1 To check if a user exists in the Linux system: getent passwd | grep username Count the number of users: getent passwd | wc -l","title":"List Users"},{"location":"Notes/Linux-in-General/#task-manager-in-ubuntu-gnome-system-monitor-or-htop","text":"","title":"Task Manager in Ubuntu: gnome-system-monitor or htop"},{"location":"Notes/Linux-in-General/#the-address-of-an-installed-application-after-sudo-apt-get-install-dpkg-l-packagename","text":"","title":"The address of an installed application after sudo apt-get install: dpkg -L &lt;packagename&gt;"},{"location":"Notes/Linux-in-General/#record-your-voice-with-microphone","text":"Check the volume of your microphone first from alsamixer Record your voice: rec test.wav Ctrl+C to finish recording","title":"Record your voice with microphone"},{"location":"Notes/Linux-in-General/#firewall-with-ufw","text":"General commands for UFW Deny ICMP ping request","title":"Firewall with UFW"},{"location":"Notes/Python-in-General/","text":"Python in General Useful information about libraries Change Jupyter notebook starting directory: jupyter notebook --notebook-dir=D: Matplotlib Customizing style sheets Before Using Style: import numpy as np import matplotlib.pyplot as plt data = np.random.randn(50) plt.plot(data) plt.show() After Using Style: plt.style.use('ggplot') plt.plot(data) plt.show() To see avaliable styles: plt.style.available Visualization with Pandas import pandas as pd import seaborn as sns import numpy as np data = pd.DataFrame(np.random.rand(10, 4), columns=['A','B','C','D']) data A B C D 0 0.271686 0.105439 0.473730 0.490360 1 0.901196 0.785185 0.789935 0.746689 2 0.561274 0.853798 0.442205 0.340178 3 0.967522 0.254893 0.349356 0.314970 4 0.981050 0.084075 0.652177 0.789420 5 0.330425 0.005584 0.025657 0.345348 6 0.847339 0.569559 0.668655 0.135351 7 0.564038 0.855259 0.832140 0.530637 8 0.087447 0.901836 0.190969 0.951888 9 0.554184 0.211951 0.009899 0.312860 Plot line: data.plot() Plot line with subplots: data.plot(subplots=True,figsize=(12,12)) Plot line with layout of subplots: data.plot(subplots=True,figsize=(12,12), layout=(2,2)) Columns per row barplot: data.plot.bar() Columns per row stacked bar: data.plot.bar(stacked=True) Horizontal bar (remove stacked if no need): data.plot.barh(stacked=True) Change color map before plotting if you want: sns.set_palette('magma') data.plot.barh(stacked=True) Plot the area: data.plot.area() Area is plotted stacked in default. Without stack: data.plot.area(stacked=False, alpha=0.5) Plot distribution: data.plot.kde() Plot scatter: data.plot.scatter(x='A',y='B', #scatterplot x and y c='red', #color of data points s=data['B']*200) #size of data points respect to values of B column DataFrame with billions of rows: Vaex Vaex is DataFrame library just like Pandas but waaaay better than Pandas if we have billions of rows. GitHub Page Introduction","title":"Python in General"},{"location":"Notes/Python-in-General/#python-in-general","text":"Useful information about libraries Change Jupyter notebook starting directory: jupyter notebook --notebook-dir=D:","title":"Python in General"},{"location":"Notes/Python-in-General/#matplotlib","text":"Customizing style sheets Before Using Style: import numpy as np import matplotlib.pyplot as plt data = np.random.randn(50) plt.plot(data) plt.show() After Using Style: plt.style.use('ggplot') plt.plot(data) plt.show() To see avaliable styles: plt.style.available","title":"Matplotlib"},{"location":"Notes/Python-in-General/#visualization-with-pandas","text":"import pandas as pd import seaborn as sns import numpy as np data = pd.DataFrame(np.random.rand(10, 4), columns=['A','B','C','D']) data A B C D 0 0.271686 0.105439 0.473730 0.490360 1 0.901196 0.785185 0.789935 0.746689 2 0.561274 0.853798 0.442205 0.340178 3 0.967522 0.254893 0.349356 0.314970 4 0.981050 0.084075 0.652177 0.789420 5 0.330425 0.005584 0.025657 0.345348 6 0.847339 0.569559 0.668655 0.135351 7 0.564038 0.855259 0.832140 0.530637 8 0.087447 0.901836 0.190969 0.951888 9 0.554184 0.211951 0.009899 0.312860 Plot line: data.plot() Plot line with subplots: data.plot(subplots=True,figsize=(12,12)) Plot line with layout of subplots: data.plot(subplots=True,figsize=(12,12), layout=(2,2)) Columns per row barplot: data.plot.bar() Columns per row stacked bar: data.plot.bar(stacked=True) Horizontal bar (remove stacked if no need): data.plot.barh(stacked=True) Change color map before plotting if you want: sns.set_palette('magma') data.plot.barh(stacked=True) Plot the area: data.plot.area() Area is plotted stacked in default. Without stack: data.plot.area(stacked=False, alpha=0.5) Plot distribution: data.plot.kde() Plot scatter: data.plot.scatter(x='A',y='B', #scatterplot x and y c='red', #color of data points s=data['B']*200) #size of data points respect to values of B column","title":"Visualization with Pandas"},{"location":"Notes/Python-in-General/#dataframe-with-billions-of-rows-vaex","text":"Vaex is DataFrame library just like Pandas but waaaay better than Pandas if we have billions of rows. GitHub Page Introduction","title":"DataFrame with billions of rows: Vaex"},{"location":"Notes/Reinforcement-Learning/","text":"Reinforcement Learning","title":"Reinforcement Learning"},{"location":"Notes/Reinforcement-Learning/#reinforcement-learning","text":"","title":"Reinforcement Learning"},{"location":"Projects/Failed-Attempts-of-ASR/","text":"ASR Failed Attempts This project is about promoting Taiwanese aboriginal languages with finding the similarity of pronunciation between proper pronunciation and recorded sound from client. A similar project: Speechace Info for Audio Signals Tutorial for: MFCC Audio Analysis with Python: pyAudioAnalysis Bandpass filter: Butterworth filter Cosine similarity Euclidean vs. Cosine Distance Attemp 1: Audio Fingerprinting What is audio fingerprinting: https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/ Another general information about audio fingerprinting: https://medium.com/intrasonics/a-fingerprint-for-audio-3b337551a671 Visual representation of audio fingerprinting: PyDataNYC2015 Audio Fingerprinting Library for Python 3.6: Dejavu Python 2 version: Dejavu python 2.x Usage of MYSQL: Python MYSQL Another Library for Python: https://github.com/itspoma/audio-fingerprint-identifying-python Example with the library: Example Another Library for Python: audfprint Tool for audio fingerprinting: compariSong API from Google: musicg Attemp 2: Image Processing Fundamental idea of using image processing: Answer on StackOverFlow Blob detection example: Github link Attempt 3: Dynamic Time Warping Most popular DTW library for Python: DTW An example with Python library: GitHub Link Another library for Python: tslearn DTAIDistance","title":"ASR Failed Attempts"},{"location":"Projects/Failed-Attempts-of-ASR/#asr-failed-attempts","text":"This project is about promoting Taiwanese aboriginal languages with finding the similarity of pronunciation between proper pronunciation and recorded sound from client. A similar project: Speechace","title":"ASR Failed Attempts"},{"location":"Projects/Failed-Attempts-of-ASR/#info-for-audio-signals","text":"Tutorial for: MFCC Audio Analysis with Python: pyAudioAnalysis Bandpass filter: Butterworth filter Cosine similarity Euclidean vs. Cosine Distance","title":"Info for Audio Signals"},{"location":"Projects/Failed-Attempts-of-ASR/#attemp-1-audio-fingerprinting","text":"What is audio fingerprinting: https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/ Another general information about audio fingerprinting: https://medium.com/intrasonics/a-fingerprint-for-audio-3b337551a671 Visual representation of audio fingerprinting: PyDataNYC2015 Audio Fingerprinting Library for Python 3.6: Dejavu Python 2 version: Dejavu python 2.x Usage of MYSQL: Python MYSQL Another Library for Python: https://github.com/itspoma/audio-fingerprint-identifying-python Example with the library: Example Another Library for Python: audfprint Tool for audio fingerprinting: compariSong API from Google: musicg","title":"Attemp 1: Audio Fingerprinting"},{"location":"Projects/Failed-Attempts-of-ASR/#attemp-2-image-processing","text":"Fundamental idea of using image processing: Answer on StackOverFlow Blob detection example: Github link","title":"Attemp 2: Image Processing"},{"location":"Projects/Failed-Attempts-of-ASR/#attempt-3-dynamic-time-warping","text":"Most popular DTW library for Python: DTW An example with Python library: GitHub Link Another library for Python: tslearn DTAIDistance","title":"Attempt 3: Dynamic Time Warping"},{"location":"Projects/Frog-Classification/","text":"Frog Classification Classification of frog species from audio signals. Dataset for audio signals: Dataset An example project: Environmental Sound Classification It is not a good project. It uses the mean of each feature. It is not 2D CNN A better example from Aqib Saeed: Urban Sound Classification Info for Audio Signals, Dataset and Models Tutorial for: MFCC Norwegian guy: JonNor Dataset for Environmental Sound Classification: ESC-50 A CNN model for sound classifications: SBCNN Augmentation Animal Classification : using stacked MFCCs = MFCC, MFCC delta, MFCC delta 2 Pre-trained model by Google: AudioSet","title":"Frog Classification"},{"location":"Projects/Frog-Classification/#frog-classification","text":"Classification of frog species from audio signals. Dataset for audio signals: Dataset An example project: Environmental Sound Classification It is not a good project. It uses the mean of each feature. It is not 2D CNN A better example from Aqib Saeed: Urban Sound Classification","title":"Frog Classification"},{"location":"Projects/Frog-Classification/#info-for-audio-signals-dataset-and-models","text":"Tutorial for: MFCC Norwegian guy: JonNor Dataset for Environmental Sound Classification: ESC-50 A CNN model for sound classifications: SBCNN Augmentation Animal Classification : using stacked MFCCs = MFCC, MFCC delta, MFCC delta 2 Pre-trained model by Google: AudioSet","title":"Info for Audio Signals, Dataset and Models"},{"location":"Projects/Kaggle-Global-Wheat-Detection/","text":"Kaggle Global Wheat Detection Identify wheat heads using object detection Object Detection Models: YOLOv4 : Start with this first Detectron 2 : Object detection with Pytorch mmdetection TensorPack TensorFlow Object Detection API","title":"Kaggle Global Wheat Detection"},{"location":"Projects/Kaggle-Global-Wheat-Detection/#kaggle-global-wheat-detection","text":"Identify wheat heads using object detection","title":"Kaggle Global Wheat Detection"},{"location":"Projects/Kaggle-Global-Wheat-Detection/#object-detection-models","text":"YOLOv4 : Start with this first Detectron 2 : Object detection with Pytorch mmdetection TensorPack TensorFlow Object Detection API","title":"Object Detection Models:"},{"location":"Projects/Kaldi-ASR/","text":"Kaldi-ASR This project is about promoting Taiwanese aboriginal languages with finding the similarity of pronunciation between proper pronunciation and recorded sound from client. A similar project: Speechace Info for Audio Signals Tutorial for: MFCC Audio Analysis with Python: pyAudioAnalysis Bandpass filter: Butterworth filter Cosine similarity Euclidean vs. Cosine Distance Info for ASR Good notes from a Czech guy: Link Forced Alignment: http://mirlab.org/jang/books/audiosignalProcessing/speechAssessment.asp?title=8-3%20%BBy%AD%B5%B5%FB%A4%C0 Better info about Forced Alignment: Forced Alignment Presentation Old School HMM Toolkit: HTK Better HMM Toolkit: Kaldi Another Speech recognition: Julius General information about Kaldi: Kaldi as ASR How to start with Kaldi: Kaldi and Speech Recognition Lectures for Kaldi: Kaldi Lectures Open Speech Dataset: VoxForge Another Open Dataset: CLARIN Paper to read: The SRI EduSpeak System: Recognition and Pronunciation Scoring for Language Learning Speech Recognition vs Force Alignment A speech recognition system uses a search engine along with an acoustic and language model which contains a set of possible words, phonemes, or some other set of data to match speech data to the correct spoken utterance. The search engine processes the features extracted from the speech data to identify occurences of the words, phonemes, or whatever set of data it is equipped to search for and returns the results. Forced alignment is similar to this process, but it differs in one major respect. Rather than being given a set of possible words to search for, the search engine is given an exact transcription of what is being spoken in the speech data. The system then aligns the transcribed data with the speech data, identifying which time segments in the speech data correspond to particular words in the transcription data. Forced alignment can also be used to align the phonemes of the transcription data to the speech data given, similar to the image below, although with more explicitly defined boundaries on where each phoneme begins and ends.","title":"Kaldi-ASR"},{"location":"Projects/Kaldi-ASR/#kaldi-asr","text":"This project is about promoting Taiwanese aboriginal languages with finding the similarity of pronunciation between proper pronunciation and recorded sound from client. A similar project: Speechace","title":"Kaldi-ASR"},{"location":"Projects/Kaldi-ASR/#info-for-audio-signals","text":"Tutorial for: MFCC Audio Analysis with Python: pyAudioAnalysis Bandpass filter: Butterworth filter Cosine similarity Euclidean vs. Cosine Distance","title":"Info for Audio Signals"},{"location":"Projects/Kaldi-ASR/#info-for-asr","text":"Good notes from a Czech guy: Link Forced Alignment: http://mirlab.org/jang/books/audiosignalProcessing/speechAssessment.asp?title=8-3%20%BBy%AD%B5%B5%FB%A4%C0 Better info about Forced Alignment: Forced Alignment Presentation Old School HMM Toolkit: HTK Better HMM Toolkit: Kaldi Another Speech recognition: Julius General information about Kaldi: Kaldi as ASR How to start with Kaldi: Kaldi and Speech Recognition Lectures for Kaldi: Kaldi Lectures Open Speech Dataset: VoxForge Another Open Dataset: CLARIN Paper to read: The SRI EduSpeak System: Recognition and Pronunciation Scoring for Language Learning","title":"Info for ASR"},{"location":"Projects/Kaldi-ASR/#speech-recognition-vs-force-alignment","text":"A speech recognition system uses a search engine along with an acoustic and language model which contains a set of possible words, phonemes, or some other set of data to match speech data to the correct spoken utterance. The search engine processes the features extracted from the speech data to identify occurences of the words, phonemes, or whatever set of data it is equipped to search for and returns the results. Forced alignment is similar to this process, but it differs in one major respect. Rather than being given a set of possible words to search for, the search engine is given an exact transcription of what is being spoken in the speech data. The system then aligns the transcribed data with the speech data, identifying which time segments in the speech data correspond to particular words in the transcription data. Forced alignment can also be used to align the phonemes of the transcription data to the speech data given, similar to the image below, although with more explicitly defined boundaries on where each phoneme begins and ends.","title":"Speech Recognition vs Force Alignment"}]}